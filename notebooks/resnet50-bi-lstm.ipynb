{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-04T17:49:24.697730Z",
     "iopub.status.busy": "2025-06-04T17:49:24.697283Z",
     "iopub.status.idle": "2025-06-04T17:49:24.721692Z",
     "shell.execute_reply": "2025-06-04T17:49:24.720918Z",
     "shell.execute_reply.started": "2025-06-04T17:49:24.697702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/handwriting-hdf5/data/washington.hdf5\n",
      "/kaggle/input/handwriting-hdf5/data/bentham.hdf5\n",
      "/kaggle/input/handwriting-hdf5/data/iam.hdf5\n",
      "/kaggle/input/handwriting-hdf5/data/saintgall.hdf5\n",
      "/kaggle/input/handwriting-hdf5/data/merged/merged.hdf5\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:49:27.659631Z",
     "iopub.status.busy": "2025-06-04T17:49:27.659382Z",
     "iopub.status.idle": "2025-06-04T17:49:27.663686Z",
     "shell.execute_reply": "2025-06-04T17:49:27.662987Z",
     "shell.execute_reply.started": "2025-06-04T17:49:27.659611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('/kaggle/working/experimental-transformer-ocr'):\n",
    "    ! git clone https://github.com/aritra-github26/experimental-transformer-ocr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:49:28.599557Z",
     "iopub.status.busy": "2025-06-04T17:49:28.599302Z",
     "iopub.status.idle": "2025-06-04T17:49:28.718764Z",
     "shell.execute_reply": "2025-06-04T17:49:28.717897Z",
     "shell.execute_reply.started": "2025-06-04T17:49:28.599537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/experimental-transformer-ocr\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/kaggle/working/experimental-transformer-ocr')\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:49:30.650262Z",
     "iopub.status.busy": "2025-06-04T17:49:30.649617Z",
     "iopub.status.idle": "2025-06-04T17:50:44.642482Z",
     "shell.execute_reply": "2025-06-04T17:50:44.641792Z",
     "shell.execute_reply.started": "2025-06-04T17:49:30.650233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.60.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.11.0.86)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.21.0+cu124)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.13.0)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.8.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->-r requirements.txt (line 4)) (0.43.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 10))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 12)) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 10)) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r requirements.txt (line 2)) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:44.644027Z",
     "iopub.status.busy": "2025-06-04T17:50:44.643710Z",
     "iopub.status.idle": "2025-06-04T17:50:44.760734Z",
     "shell.execute_reply": "2025-06-04T17:50:44.760067Z",
     "shell.execute_reply.started": "2025-06-04T17:50:44.644000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/experimental-transformer-ocr/src\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/kaggle/working/experimental-transformer-ocr/src')\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:44.762379Z",
     "iopub.status.busy": "2025-06-04T17:50:44.762086Z",
     "iopub.status.idle": "2025-06-04T17:50:52.200823Z",
     "shell.execute_reply": "2025-06-04T17:50:52.200005Z",
     "shell.execute_reply.started": "2025-06-04T17:50:44.762336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import groupby\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, resnet34\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from data import preproc as pp\n",
    "from data import evaluation\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:52.203140Z",
     "iopub.status.busy": "2025-06-04T17:50:52.202748Z",
     "iopub.status.idle": "2025-06-04T17:50:52.214659Z",
     "shell.execute_reply": "2025-06-04T17:50:52.214087Z",
     "shell.execute_reply.started": "2025-06-04T17:50:52.203121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class OCR(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50(pretrained=True)  # Use pretrained weights\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # Add batch normalization after CNN features\n",
    "        self.batch_norm = nn.BatchNorm2d(2048)\n",
    "        \n",
    "        # create conversion layer with dropout\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "        \n",
    "        # Add layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # create a 2-layer BiLSTM with dropout\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=2,\n",
    "                           bidirectional=True, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Add dropout before final projection\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        # prediction heads with length of vocab\n",
    "        self.vocab = nn.Linear(hidden_dim * 2, vocab_len)\n",
    "\n",
    "        # spatial positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.query_pos = PositionalEncoding(hidden_dim, 0.1)  # Reduced dropout\n",
    "\n",
    "    def get_feature(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)   \n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the model with improved regularization.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input images (batch of images).\n",
    "        \n",
    "        Returns:\n",
    "            Output predictions for the sequences.\n",
    "        \"\"\"\n",
    "        # Propagate inputs through ResNet-50\n",
    "        x = self.get_feature(inputs)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Apply dropout and conv\n",
    "        x = self.dropout1(x)\n",
    "        h = self.conv(x)  # shape: (batch, hidden_dim, H, W)\n",
    "\n",
    "        # Add spatial positional encodings\n",
    "        h_shape = h.shape\n",
    "        H, W = h_shape[2], h_shape[3]\n",
    "        row_emb = self.row_embed[:H].unsqueeze(1).repeat(1, W, 1)\n",
    "        col_emb = self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1)\n",
    "        pos_emb = torch.cat([row_emb, col_emb], dim=-1).permute(2, 0, 1).unsqueeze(0)\n",
    "        pos_emb = pos_emb.to(h.device)\n",
    "        h = h + pos_emb\n",
    "\n",
    "        # Prepare input for LSTM\n",
    "        h = h.flatten(2).permute(0, 2, 1)  # (batch, seq_len, feature)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        h = self.layer_norm(h)\n",
    "\n",
    "        # Add positional encoding\n",
    "        h = h.permute(1, 0, 2)  # (seq_len, batch, feature)\n",
    "        h = self.query_pos(h)\n",
    "        h = h.permute(1, 0, 2)  # (batch, seq_len, feature)\n",
    "\n",
    "        # Pass through BiLSTM\n",
    "        h, _ = self.lstm(h)  # (batch, seq_len, hidden_dim*2)\n",
    "        \n",
    "        # Apply dropout before final projection\n",
    "        h = self.dropout2(h)\n",
    "        \n",
    "        # Calculate output with temperature scaling\n",
    "        temperature = 0.1\n",
    "        h = h.permute(1, 0, 2)  # (seq_len, batch, hidden_dim*2)\n",
    "        output = self.vocab(h) / temperature  # Scale logits\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def make_model(vocab_len, hidden_dim=256):\n",
    "    \n",
    "    return OCR(vocab_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_batch(dataloader):\n",
    "    batch = next(iter(dataloader))\n",
    "    imgs, labels = batch\n",
    "    \n",
    "    print(\"Batch shapes:\")\n",
    "    print(f\"Images: {imgs.shape}\")\n",
    "    print(f\"Labels: {labels.shape}\")\n",
    "    \n",
    "    # Show first image and its label\n",
    "    img = imgs[0].numpy().transpose(1, 2, 0)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Ground Truth: {tokenizer.decode(labels[0].tolist())}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print first few labels\n",
    "    print(\"\\nFirst 3 decoded labels:\")\n",
    "    for i in range(min(3, len(labels))):\n",
    "        print(f\"{i+1}. {tokenizer.decode(labels[i].tolist())}\")\n",
    "\n",
    "print(\"Inspecting training data:\")\n",
    "inspect_batch(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated training configuration\n",
    "import os\n",
    "import datetime\n",
    "import string\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32  # Increased for better batch statistics\n",
    "epochs = 50      # Reduced but with better scheduling\n",
    "warmup_epochs = 3\n",
    "max_lr = 0.001\n",
    "min_lr = 1e-6\n",
    "\n",
    "# Gradient accumulation for effective larger batch size\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# define paths\n",
    "source_path = '/kaggle/input/handwriting-hdf5/data/merged/merged.hdf5'\n",
    "output_path = '/kaggle/working/output'\n",
    "target_path = output_path + '/merged_training_weights_ctc_150.pt'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "charset_base = string.printable[:95]\n",
    "\n",
    "print(\"Source:\", source_path)\n",
    "print(\"Output:\", output_path)\n",
    "print(\"Target:\", target_path)\n",
    "print(\"Charset:\", charset_base)\n",
    "print(f\"Training config: batch_size={batch_size}, epochs={epochs}, warmup_epochs={warmup_epochs}\")\n",
    "print(f\"Learning rates: max_lr={max_lr}, min_lr={min_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:52.215447Z",
     "iopub.status.busy": "2025-06-04T17:50:52.215273Z",
     "iopub.status.idle": "2025-06-04T17:50:52.972646Z",
     "shell.execute_reply": "2025-06-04T17:50:52.971974Z",
     "shell.execute_reply.started": "2025-06-04T17:50:52.215432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uses generator functions to supply train/test with data.\n",
    "Image renderings and text are created on the fly each time.\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    \"\"\"Generator class with data streaming\"\"\"\n",
    "\n",
    "    def __init__(self, source,charset, max_text_length, split, transform):\n",
    "        self.tokenizer = Tokenizer(charset, max_text_length)\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.split = split\n",
    "        self.dataset = dict()\n",
    "\n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            self.dataset[self.split] = dict()\n",
    "\n",
    "            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n",
    "            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n",
    "          \n",
    "            randomize = np.arange(len(self.dataset[self.split]['gt']))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(randomize)\n",
    "\n",
    "            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n",
    "            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n",
    "\n",
    "            # decode sentences from byte\n",
    "            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n",
    "            \n",
    "        self.size = len(self.dataset[self.split]['gt'])\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = self.dataset[self.split]['dt'][i]\n",
    "        \n",
    "        #making image compatible with resnet\n",
    "        img = np.repeat(img[..., np.newaxis],3, -1)    \n",
    "        img = pp.normalization(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n",
    "        \n",
    "        #padding till max length\n",
    "        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n",
    "\n",
    "        gt = torch.Tensor(y_train)\n",
    "\n",
    "        return img, gt          \n",
    "\n",
    "    def __len__(self):\n",
    "      return self.size\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer():\n",
    "    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n",
    "\n",
    "    def __init__(self, chars, max_text_length=128):\n",
    "        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n",
    "        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n",
    "        self.PAD = self.chars.index(self.PAD_TK)\n",
    "        self.UNK = self.chars.index(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_text_length\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to vector\"\"\"\n",
    "\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        groups = [\"\".join(group) for _, group in groupby(text)]\n",
    "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
    "        encoded = []\n",
    "\n",
    "        text = ['SOS'] + list(text) + ['EOS']\n",
    "        for item in text:\n",
    "            index = self.chars.index(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "\n",
    "        return np.asarray(encoded)\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "        \n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:52.973532Z",
     "iopub.status.busy": "2025-06-04T17:50:52.973341Z",
     "iopub.status.idle": "2025-06-04T17:50:52.989814Z",
     "shell.execute_reply": "2025-06-04T17:50:52.989223Z",
     "shell.execute_reply.started": "2025-06-04T17:50:52.973516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: /kaggle/input/handwriting-hdf5/data/merged/merged.hdf5\n",
      "output: /kaggle/working/output\n",
      "target /kaggle/working/output/merged_training_weights_ctc_150.pt\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "\n",
    "# define paths\n",
    "#change paths accordingly\n",
    "\n",
    "source_path = '/kaggle/input/handwriting-hdf5/data/merged/merged.hdf5'\n",
    "output_path = '/kaggle/working/output'\n",
    "target_path = output_path + '/merged_training_weights_ctc_150.pt'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "charset_base = string.printable[:95]\n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"output:\", output_path)\n",
    "print(\"target\", target_path)\n",
    "print(\"charset:\", charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:52.990779Z",
     "iopub.status.busy": "2025-06-04T17:50:52.990543Z",
     "iopub.status.idle": "2025-06-04T17:50:53.003856Z",
     "shell.execute_reply": "2025-06-04T17:50:53.003356Z",
     "shell.execute_reply.started": "2025-06-04T17:50:52.990758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()])\n",
    "tokenizer = Tokenizer(charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T16:27:18.899063Z",
     "iopub.status.busy": "2025-06-04T16:27:18.898809Z",
     "iopub.status.idle": "2025-06-04T16:27:33.797317Z",
     "shell.execute_reply": "2025-06-04T16:27:33.796743Z",
     "shell.execute_reply.started": "2025-06-04T16:27:18.899038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(DataGenerator(source_path,charset_base,max_text_length,'train',transform), batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(DataGenerator(source_path,charset_base,max_text_length,'valid',transform), batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T16:50:10.451788Z",
     "iopub.status.busy": "2025-06-04T16:50:10.451533Z",
     "iopub.status.idle": "2025-06-04T16:50:10.494052Z",
     "shell.execute_reply": "2025-06-04T16:50:10.493173Z",
     "shell.execute_reply.started": "2025-06-04T16:50:10.451771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/1703023475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model = make_model(vocab_len=tokenizer.vocab_size)\n",
    "_=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop with improved monitoring\n",
    "best_valid_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "early_stopping_counter = 0\n",
    "training_history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training on device: {device}\")\n",
    "print(f\"Batch size: {batch_size} (effective batch size: {batch_size * gradient_accumulation_steps})\")\n",
    "print(f\"Number of epochs: {epochs}\")\n",
    "print(f\"Learning rate range: {min_lr} to {max_lr}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss = train(model, criterion, optimizer, scheduler, \n",
    "                      train_loader, tokenizer.vocab_size, device)\n",
    "    \n",
    "    # Validation phase\n",
    "    valid_loss = evaluate(model, criterion, val_loader, tokenizer.vocab_size, device)\n",
    "    \n",
    "    # Update learning rate schedulers\n",
    "    scheduler.step()  # Update the warmup/decay scheduler\n",
    "    plateau_scheduler.step(valid_loss)  # Update the plateau scheduler\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Save best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_valid_loss': best_valid_loss,\n",
    "            'tokenizer': tokenizer,\n",
    "        }, target_path)\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Store training history\n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'valid_loss': valid_loss,\n",
    "        'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "        'time': epoch_mins * 60 + epoch_secs\n",
    "    })\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f'\\nEpoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.4f}')\n",
    "    print(f'\\tLearning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "    \n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Save training history\n",
    "import pandas as pd\n",
    "history_df = pd.DataFrame(training_history)\n",
    "history_df.to_csv(f'{output_path}/training_history.csv', index=False)\n",
    "\n",
    "print(f'\\nTraining completed. Best validation loss: {best_valid_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T16:27:34.517078Z",
     "iopub.status.busy": "2025-06-04T16:27:34.516804Z",
     "iopub.status.idle": "2025-06-04T16:27:34.522354Z",
     "shell.execute_reply": "2025-06-04T16:27:34.521475Z",
     "shell.execute_reply.started": "2025-06-04T16:27:34.517059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss function with blank token at index 0\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "criterion.to(device)\n",
    "\n",
    "# Optimizer with improved weight decay and initial learning rate\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=min_lr,  # Start with min_lr for warmup\n",
    "    weight_decay=0.01,  # Increased weight decay for better regularization\n",
    "    betas=(0.9, 0.999),  # Default Adam betas\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup and plateau detection\n",
    "def get_lr(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        return min_lr + (max_lr - min_lr) * epoch / warmup_epochs\n",
    "    return max_lr * (0.95 ** (epoch - warmup_epochs))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr)\n",
    "\n",
    "# Additional plateau scheduler for fine-tuning\n",
    "plateau_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    min_lr=min_lr\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:53.004718Z",
     "iopub.status.busy": "2025-06-04T17:50:53.004537Z",
     "iopub.status.idle": "2025-06-04T17:50:53.030576Z",
     "shell.execute_reply": "2025-06-04T17:50:53.029860Z",
     "shell.execute_reply.started": "2025-06-04T17:50:53.004704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, criterion, optimizer, scheduler, dataloader, vocab_length, device):\n",
    "    \"\"\"\n",
    "    Train the model using the provided dataloader with gradient accumulation.\n",
    "    \n",
    "    Args:\n",
    "        model: The OCR model\n",
    "        criterion: Loss function (CTC)\n",
    "        optimizer: Optimizer instance\n",
    "        scheduler: Learning rate scheduler\n",
    "        dataloader: Training data loader\n",
    "        vocab_length: Size of vocabulary\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    optimizer.zero_grad()  # Zero gradients at start of epoch\n",
    "    \n",
    "    for batch_idx, (imgs, labels_y) in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        labels_y = labels_y.to(device)\n",
    "        batch_size = imgs.size(0)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(imgs.float())\n",
    "        \n",
    "        # Ensure output is in (batch, seq_len, vocab_size) format\n",
    "        if output.dim() != 3:\n",
    "            raise ValueError(f\"Expected 3D output tensor, got shape: {output.shape}\")\n",
    "        \n",
    "        # Apply temperature scaling and log_softmax\n",
    "        temperature = 0.1\n",
    "        log_probs = F.log_softmax(output / temperature, dim=2).permute(1, 0, 2)\n",
    "        \n",
    "        # Calculate input sequence lengths\n",
    "        input_lengths = torch.full((batch_size,), \n",
    "                                 log_probs.size(0), \n",
    "                                 dtype=torch.long,\n",
    "                                 device=device)\n",
    "        \n",
    "        # Process target sequences\n",
    "        target_lengths = []\n",
    "        labels_list = []\n",
    "        valid_samples = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Find non-zero elements (non-padding)\n",
    "            non_zero = labels_y[i].nonzero().squeeze()\n",
    "            if non_zero.dim() == 0:\n",
    "                continue\n",
    "            \n",
    "            length = non_zero.shape[0]\n",
    "            if length > log_probs.size(0):\n",
    "                continue\n",
    "                \n",
    "            sequence = labels_y[i, :length]\n",
    "            target_lengths.append(length)\n",
    "            labels_list.append(sequence)\n",
    "            valid_samples.append(i)\n",
    "        \n",
    "        if not valid_samples:\n",
    "            continue\n",
    "            \n",
    "        # Keep only valid samples\n",
    "        valid_samples = torch.tensor(valid_samples, device=device)\n",
    "        log_probs = log_probs[:, valid_samples, :]\n",
    "        input_lengths = input_lengths[valid_samples]\n",
    "        target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "        labels_packed = torch.cat(labels_list)\n",
    "        \n",
    "        try:\n",
    "            # Calculate CTC loss\n",
    "            loss = criterion(log_probs,\n",
    "                           labels_packed,\n",
    "                           input_lengths,\n",
    "                           target_lengths)\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights if we've accumulated enough gradients\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Track statistics\n",
    "            total_loss += loss.item() * len(valid_samples) * gradient_accumulation_steps\n",
    "            total_items += len(valid_samples)\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                avg_loss = total_loss / total_items if total_items > 0 else float('inf')\n",
    "                print(f'Batch {batch_idx + 1}/{len(dataloader)}, '\n",
    "                      f'Loss: {avg_loss:.4f}, '\n",
    "                      f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in batch {batch_idx}:\")\n",
    "            print(f\"log_probs shape: {log_probs.shape}\")\n",
    "            print(f\"labels_packed shape: {labels_packed.shape}\")\n",
    "            print(f\"input_lengths shape: {input_lengths.shape}\")\n",
    "            print(f\"target_lengths shape: {target_lengths.shape}\")\n",
    "            raise e\n",
    "        \n",
    "        # Clear memory\n",
    "        del output, log_probs, loss\n",
    "        if batch_idx % 10 == 0:  # Every 10 batches\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Handle any remaining accumulated gradients\n",
    "    if (batch_idx + 1) % gradient_accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / total_items if total_items > 0 else float('inf')\n",
    "\n",
    "def evaluate(model, criterion, dataloader, vocab_length, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model using the provided dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model: The OCR model\n",
    "        criterion: Loss function (CTC)\n",
    "        dataloader: Validation data loader\n",
    "        vocab_length: Size of vocabulary\n",
    "        device: Device to evaluate on\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (imgs, labels_y) in enumerate(dataloader):\n",
    "            imgs = imgs.to(device)\n",
    "            labels_y = labels_y.to(device)\n",
    "            batch_size = imgs.size(0)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(imgs.float())\n",
    "            \n",
    "            # Ensure output is in (batch, seq_len, vocab_size) format\n",
    "            if output.dim() != 3:\n",
    "                raise ValueError(f\"Expected 3D output tensor, got shape: {output.shape}\")\n",
    "            \n",
    "            # Permute to (seq_len, batch, vocab_size) for CTC loss\n",
    "            log_probs = F.log_softmax(output, dim=2).permute(1, 0, 2)\n",
    "            \n",
    "            # Calculate input sequence lengths (all are same length after CNN processing)\n",
    "            input_lengths = torch.full((batch_size,), \n",
    "                                     log_probs.size(0), \n",
    "                                     dtype=torch.long,\n",
    "                                     device=device)\n",
    "            \n",
    "            # Calculate target lengths (excluding padding)\n",
    "            target_lengths = []\n",
    "            labels_list = []\n",
    "            valid_samples = []\n",
    "            \n",
    "            # Process each sequence in the batch\n",
    "            for i in range(batch_size):\n",
    "                # Find non-zero elements (non-padding)\n",
    "                non_zero = labels_y[i].nonzero().squeeze()\n",
    "                if non_zero.dim() == 0:  # Handle case of empty sequence\n",
    "                    continue  # Skip this sample\n",
    "                \n",
    "                length = non_zero.shape[0]\n",
    "                if length > log_probs.size(0):  # Skip if target is longer than output\n",
    "                    continue\n",
    "                    \n",
    "                sequence = labels_y[i, :length]\n",
    "                target_lengths.append(length)\n",
    "                labels_list.append(sequence)\n",
    "                valid_samples.append(i)\n",
    "            \n",
    "            # Skip batch if no valid samples\n",
    "            if not valid_samples:\n",
    "                continue\n",
    "                \n",
    "            # Keep only valid samples\n",
    "            valid_samples = torch.tensor(valid_samples, device=device)\n",
    "            log_probs = log_probs[:, valid_samples, :]\n",
    "            input_lengths = input_lengths[valid_samples]\n",
    "            \n",
    "            # Convert target lengths to tensor\n",
    "            target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n",
    "            \n",
    "            # Concatenate all label sequences\n",
    "            labels_packed = torch.cat(labels_list)\n",
    "            \n",
    "            try:\n",
    "                # CTC loss calculation\n",
    "                loss = criterion(log_probs,\n",
    "                               labels_packed,\n",
    "                               input_lengths,\n",
    "                               target_lengths)\n",
    "                \n",
    "                total_loss += loss.item() * len(valid_samples)\n",
    "                total_items += len(valid_samples)\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch {batch}:\")\n",
    "                print(f\"log_probs shape: {log_probs.shape}\")\n",
    "                print(f\"labels_packed shape: {labels_packed.shape}\")\n",
    "                print(f\"input_lengths shape: {input_lengths.shape}\")\n",
    "                print(f\"target_lengths shape: {target_lengths.shape}\")\n",
    "                raise e\n",
    "\n",
    "    return total_loss / total_items if total_items > 0 else float('inf')\n",
    "\n",
    "def get_memory(model, imgs):\n",
    "    \"\"\"\n",
    "    Extract features and apply positional encoding for the BiLSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: The OCR model\n",
    "        imgs: Input images tensor\n",
    "        \n",
    "    Returns:\n",
    "        Memory tensor with shape (seq_len, batch, hidden_dim*2)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Extract CNN features\n",
    "        features = model.get_feature(imgs)\n",
    "        \n",
    "        # Apply conv layer\n",
    "        conv_out = model.conv(features)\n",
    "        \n",
    "        # Get spatial dimensions\n",
    "        bs, c, h, w = conv_out.size()\n",
    "        \n",
    "        # Add positional encodings\n",
    "        row_emb = model.row_embed[:h].unsqueeze(1).repeat(1, w, 1)  # (H, W, hidden_dim//2)\n",
    "        col_emb = model.col_embed[:w].unsqueeze(0).repeat(h, 1, 1)  # (H, W, hidden_dim//2)\n",
    "        pos_emb = torch.cat([row_emb, col_emb], dim=-1).permute(2, 0, 1).unsqueeze(0)  # (1, hidden_dim, H, W)\n",
    "        pos_emb = pos_emb.to(conv_out.device)\n",
    "        conv_out = conv_out + pos_emb\n",
    "        \n",
    "        # Flatten spatial dimensions and permute for LSTM\n",
    "        lstm_input = conv_out.flatten(2).permute(0, 2, 1)  # (batch, seq_len, feature)\n",
    "        \n",
    "        # Add positional encoding to LSTM input\n",
    "        lstm_input = lstm_input.permute(1, 0, 2)  # (seq_len, batch, feature)\n",
    "        lstm_input = model.query_pos(lstm_input)\n",
    "        lstm_input = lstm_input.permute(1, 0, 2)  # (batch, seq_len, feature)\n",
    "        \n",
    "        # Apply BiLSTM\n",
    "        lstm_out, _ = model.lstm(lstm_input)\n",
    "        \n",
    "        # Return in shape (seq_len, batch, hidden_dim*2)\n",
    "        return lstm_out.permute(1, 0, 2)\n",
    "\n",
    "def single_image_inference(model, img, tokenizer, transform, device):\n",
    "    \"\"\"\n",
    "    Run inference on single image using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        model: The OCR model\n",
    "        img: Input image\n",
    "        tokenizer: Tokenizer for encoding/decoding text\n",
    "        transform: Image transform pipeline\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        pred_text: Predicted text string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess image\n",
    "    img = transform(img)\n",
    "    imgs = img.unsqueeze(0).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output = model(imgs)\n",
    "        \n",
    "        # Ensure output is in (seq_len, batch, vocab_size) format\n",
    "        if output.dim() == 3:\n",
    "            output = output.permute(1, 0, 2)\n",
    "        \n",
    "        # Apply log softmax and get predictions\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.argmax(dim=2)\n",
    "        output = output.squeeze(1)  # Remove batch dimension\n",
    "        \n",
    "        # Convert prediction to text (handle special tokens)\n",
    "        out_indices = []\n",
    "        for idx in output:\n",
    "            token = idx.item()\n",
    "            if token == tokenizer.chars.index('EOS'):\n",
    "                break\n",
    "            if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n",
    "                out_indices.append(token)\n",
    "        \n",
    "        # Decode the prediction\n",
    "        pred_text = tokenizer.decode(out_indices)\n",
    "    \n",
    "    return pred_text\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "def run_epochs(model, criterion, optimizer, scheduler, train_loader, val_loader, epochs, tokenizer, target_path, device):\n",
    "    \"\"\"\n",
    "    Run training for specified number of epochs.\n",
    "    \n",
    "    Args:\n",
    "        model: The OCR model\n",
    "        criterion: Loss function (CTC)\n",
    "        optimizer: Optimizer instance\n",
    "        scheduler: Learning rate scheduler\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        epochs: Number of epochs to train\n",
    "        tokenizer: Tokenizer for encoding/decoding text\n",
    "        target_path: Path to save model checkpoints\n",
    "        device: Device to train on\n",
    "    \"\"\"\n",
    "    best_valid_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 4  # Number of epochs to wait before reducing learning rate\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch + 1:02} | Learning rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss = train(model, criterion, optimizer, scheduler, \n",
    "                          train_loader, tokenizer.vocab_size, device)\n",
    "        \n",
    "        # Validation phase\n",
    "        valid_loss = evaluate(model, criterion, val_loader, tokenizer.vocab_size, device)\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_valid_loss': best_valid_loss,\n",
    "            }, target_path)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        \n",
    "        # Reduce learning rate if validation loss hasn't improved\n",
    "        if patience >= max_patience:\n",
    "            scheduler.step()\n",
    "            patience = 0\n",
    "        \n",
    "        print(f'Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'Train Loss: {train_loss:.3f}')\n",
    "        print(f'Val   Loss: {valid_loss:.3f}')\n",
    "        print(f'Best Val Loss: {best_valid_loss:.3f}')\n",
    "    \n",
    "    print(f'Training completed. Best validation loss: {best_valid_loss:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T20:44:08.869456Z",
     "iopub.status.busy": "2025-05-25T20:44:08.869251Z",
     "iopub.status.idle": "2025-05-25T20:55:21.080851Z",
     "shell.execute_reply": "2025-05-25T20:55:21.080157Z",
     "shell.execute_reply.started": "2025-05-25T20:44:08.869433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 learning rate[0.0001]\n",
      "Time: 3m 44s\n",
      "Train Loss: 3.726\n",
      "Val   Loss: 3.645\n",
      "Epoch: 02 learning rate[0.0001]\n",
      "Time: 3m 42s\n",
      "Train Loss: 3.612\n",
      "Val   Loss: 3.650\n",
      "Epoch: 03 learning rate[0.0001]\n",
      "Time: 3m 44s\n",
      "Train Loss: 3.601\n",
      "Val   Loss: 3.648\n",
      "3.6454115870972754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/output/training_results.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "# This is the actual code :)\n",
    "\n",
    "\n",
    "\n",
    "import joblib\n",
    "training_results = [] # format: [training loss, validation loss, epoch time in seconds]\n",
    " \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "# def run_epochs(model, criterion, optimizer, scheduler, train_loader, val_loader, epochs, tokenizer, target_path, device):\n",
    "'''\n",
    "run one epoch for a model\n",
    "'''\n",
    "epochs = 3\n",
    "c = 0\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):     \n",
    "    print(f'Epoch: {epoch + 1:02}', 'learning rate{}'.format(scheduler.get_last_lr()))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss = train(model, criterion, optimizer, scheduler, \n",
    "                              train_loader, tokenizer.vocab_size, device)\n",
    "    \n",
    "    # Validation phase\n",
    "    valid_loss = evaluate(model, criterion, val_loader, tokenizer.vocab_size, device)\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n",
    "    \n",
    "    # Save best model based on validation loss\n",
    "    c += 1\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        end_time = time.time()\n",
    "        torch.save(model.state_dict(), target_path)\n",
    "        training_results.append((train_loss, valid_loss, int(end_time - start_time)))\n",
    "        c = 0\n",
    "    \n",
    "    if c > 4:\n",
    "        scheduler.step()\n",
    "        c = 0\n",
    "    \n",
    "    \n",
    "    print(f'Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Train Loss: {train_loss:.3f}')\n",
    "    print(f'Val   Loss: {valid_loss:.3f}')\n",
    "\n",
    "print(best_valid_loss)\n",
    "\n",
    "\n",
    "# training_results.append((train_loss, valid_loss, int(end_time - start_time)))\n",
    "joblib.dump(training_results, output_path + '/training_results.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:53.031494Z",
     "iopub.status.busy": "2025-06-04T17:50:53.031280Z",
     "iopub.status.idle": "2025-06-04T17:50:53.891299Z",
     "shell.execute_reply": "2025-06-04T17:50:53.890663Z",
     "shell.execute_reply.started": "2025-06-04T17:50:53.031478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_model(vocab_len=tokenizer.vocab_size)\n",
    "_=model.to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load(target_path))\n",
    "model.load_state_dict(torch.load(target_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:53.893604Z",
     "iopub.status.busy": "2025-06-04T17:50:53.893387Z",
     "iopub.status.idle": "2025-06-04T17:50:53.897097Z",
     "shell.execute_reply": "2025-06-04T17:50:53.896384Z",
     "shell.execute_reply.started": "2025-06-04T17:50:53.893588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_memory(model, imgs):\n",
    "#     # Refactored get_memory for resnet50-biLSTM model: simply extract features and apply conv and lstm\n",
    "#     with torch.no_grad():\n",
    "#         features = model.get_feature(imgs)\n",
    "#         conv_out = model.conv(features)\n",
    "#         bs, c, h, w = conv_out.size()\n",
    "#         # Flatten spatial dimensions and permute for LSTM input: (batch, seq_len, feature)\n",
    "#         lstm_input = conv_out.flatten(2).permute(0, 2, 1)\n",
    "#         lstm_out, _ = model.lstm(lstm_input)\n",
    "#     return lstm_out.permute(1, 0, 2)  # Return in shape (seq_len, batch, feature) for compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:53.898141Z",
     "iopub.status.busy": "2025-06-04T17:50:53.897903Z",
     "iopub.status.idle": "2025-06-04T17:50:53.915308Z",
     "shell.execute_reply": "2025-06-04T17:50:53.914563Z",
     "shell.execute_reply.started": "2025-06-04T17:50:53.898103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def test(model, test_loader, max_text_length, tokenizer):\n",
    "#     \"\"\"\n",
    "#     Evaluate and predict model with the test dataloader.\n",
    "    \n",
    "#     Args:\n",
    "#         model: The OCR model to evaluate.\n",
    "#         test_loader: DataLoader for test dataset.\n",
    "#         max_text_length: Maximum length of output sequence.\n",
    "#         tokenizer: Tokenizer for decoding output tokens.\n",
    "    \n",
    "#     Returns:\n",
    "#         predicts: List of predicted text sequences.\n",
    "#         gt: List of ground truth text sequences.\n",
    "#         imgs: List of input images.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     predicts = []\n",
    "#     gt = []\n",
    "#     imgs = []\n",
    "#     device = next(model.parameters()).device\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             src, trg = batch\n",
    "#             imgs.append(src.flatten(0,1))\n",
    "#             src = src.to(device)\n",
    "#             trg = trg.to(device)\n",
    "            \n",
    "#             # Forward pass without teacher forcing\n",
    "#             output = model(src.float())\n",
    "            \n",
    "#             # Ensure output is in (seq_len, batch, vocab_size) format\n",
    "#             if output.dim() == 3:\n",
    "#                 output = output.permute(1, 0, 2)\n",
    "            \n",
    "#             # Apply log softmax and get predictions\n",
    "#             output = F.log_softmax(output, dim=2)\n",
    "#             predictions = output.argmax(dim=2)\n",
    "            \n",
    "#             # Process each sequence in the batch\n",
    "#             for pred, target in zip(predictions.transpose(0,1), trg):\n",
    "#                 # Convert prediction to text (handle special tokens)\n",
    "#                 pred_indices = []\n",
    "#                 for idx in pred:\n",
    "#                     token = idx.item()\n",
    "#                     if token == tokenizer.chars.index('EOS'):\n",
    "#                         break\n",
    "#                     if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n",
    "#                         pred_indices.append(token)\n",
    "                \n",
    "#                 # Decode prediction\n",
    "#                 pred_text = tokenizer.decode(pred_indices)\n",
    "#                 pred_text = pred_text.replace('SOS', '').replace('EOS', '')\n",
    "#                 predicts.append(pred_text)\n",
    "                \n",
    "#                 # Convert target to text (handle special tokens)\n",
    "#                 target_indices = []\n",
    "#                 for idx in target:\n",
    "#                     token = idx.item()\n",
    "#                     if token == tokenizer.chars.index('EOS'):\n",
    "#                         break\n",
    "#                     if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n",
    "#                         target_indices.append(token)\n",
    "                \n",
    "#                 # Decode target\n",
    "#                 target_text = tokenizer.decode(target_indices)\n",
    "#                 target_text = target_text.replace('SOS', '').replace('EOS', '')\n",
    "#                 gt.append(target_text)\n",
    "    \n",
    "#     return predicts, gt, imgs\n",
    "\n",
    "\n",
    "def calculate_cer(pred_text, target_text):\n",
    "    \"\"\"Calculate Character Error Rate using Levenshtein distance.\"\"\"\n",
    "    if len(target_text) == 0:\n",
    "        return 0 if len(pred_text) == 0 else 1\n",
    "        \n",
    "    matrix = [[0 for _ in range(len(pred_text) + 1)] \n",
    "              for _ in range(len(target_text) + 1)]\n",
    "    \n",
    "    for i in range(len(target_text) + 1):\n",
    "        matrix[i][0] = i\n",
    "    for j in range(len(pred_text) + 1):\n",
    "        matrix[0][j] = j\n",
    "        \n",
    "    for i in range(1, len(target_text) + 1):\n",
    "        for j in range(1, len(pred_text) + 1):\n",
    "            if target_text[i-1] == pred_text[j-1]:\n",
    "                matrix[i][j] = matrix[i-1][j-1]\n",
    "            else:\n",
    "                matrix[i][j] = min(matrix[i-1][j-1] + 1,    # substitution\n",
    "                                 matrix[i][j-1] + 1,         # insertion\n",
    "                                 matrix[i-1][j] + 1)         # deletion\n",
    "                \n",
    "    return matrix[len(target_text)][len(pred_text)] / len(target_text)\n",
    "\n",
    "def calculate_wer(pred_text, target_text):\n",
    "    \"\"\"Calculate Word Error Rate using word-level Levenshtein distance.\"\"\"\n",
    "    pred_words = pred_text.split()\n",
    "    target_words = target_text.split()\n",
    "    \n",
    "    if len(target_words) == 0:\n",
    "        return 0 if len(pred_words) == 0 else 1\n",
    "        \n",
    "    matrix = [[0 for _ in range(len(pred_words) + 1)] \n",
    "              for _ in range(len(target_words) + 1)]\n",
    "    \n",
    "    for i in range(len(target_words) + 1):\n",
    "        matrix[i][0] = i\n",
    "    for j in range(len(pred_words) + 1):\n",
    "        matrix[0][j] = j\n",
    "        \n",
    "    for i in range(1, len(target_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if target_words[i-1] == pred_words[j-1]:\n",
    "                matrix[i][j] = matrix[i-1][j-1]\n",
    "            else:\n",
    "                matrix[i][j] = min(matrix[i-1][j-1] + 1,    # substitution\n",
    "                                 matrix[i][j-1] + 1,         # insertion\n",
    "                                 matrix[i-1][j] + 1)         # deletion\n",
    "                \n",
    "    return matrix[len(target_words)][len(pred_words)] / len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:53.916253Z",
     "iopub.status.busy": "2025-06-04T17:50:53.915998Z",
     "iopub.status.idle": "2025-06-04T17:50:53.934450Z",
     "shell.execute_reply": "2025-06-04T17:50:53.933838Z",
     "shell.execute_reply.started": "2025-06-04T17:50:53.916231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, max_text_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluate and predict model with the test dataloader.\n",
    "    Memory-efficient version that processes data in chunks.\n",
    "    \n",
    "    Args:\n",
    "        model: The OCR model to evaluate.\n",
    "        test_loader: DataLoader for test dataset.\n",
    "        max_text_length: Maximum length of output sequence.\n",
    "        tokenizer: Tokenizer for decoding output tokens.\n",
    "    \n",
    "    Returns:\n",
    "        predicts: List of predicted text sequences.\n",
    "        gt: List of ground truth text sequences.\n",
    "        imgs: List of input images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    gt = []\n",
    "    imgs = []\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Clear memory before starting\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    chunk_size = 10  # Process 10 samples at a time\n",
    "    current_chunk = {'imgs': [], 'preds': [], 'gts': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            for batch_idx, batch in enumerate(test_loader):\n",
    "                src, trg = batch\n",
    "                \n",
    "                # Store CPU version of image\n",
    "                current_chunk['imgs'].append(src.flatten(0,1).cpu())\n",
    "                \n",
    "                # Move tensors to device\n",
    "                src = src.to(device)\n",
    "                trg = trg.to(device)\n",
    "                \n",
    "                try:\n",
    "                    # Forward pass without teacher forcing\n",
    "                    output = model(src.float())\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del src\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Ensure output is in (seq_len, batch, vocab_size) format\n",
    "                    if output.dim() == 3:\n",
    "                        output = output.permute(1, 0, 2)\n",
    "                    \n",
    "                    # Apply log softmax and get predictions\n",
    "                    output = F.log_softmax(output, dim=2)\n",
    "                    predictions = output.argmax(dim=2)\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del output\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Process each sequence in the batch\n",
    "                    for pred, target in zip(predictions.transpose(0,1), trg):\n",
    "                        # Convert prediction to text (handle special tokens)\n",
    "                        pred_indices = []\n",
    "                        for idx in pred:\n",
    "                            token = idx.item()\n",
    "                            if token == tokenizer.chars.index('EOS'):\n",
    "                                break\n",
    "                            if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n",
    "                                pred_indices.append(token)\n",
    "                        \n",
    "                        # Decode prediction\n",
    "                        pred_text = tokenizer.decode(pred_indices)\n",
    "                        pred_text = pred_text.replace('SOS', '').replace('EOS', '')\n",
    "                        current_chunk['preds'].append(pred_text)\n",
    "                        \n",
    "                        # Convert target to text (handle special tokens)\n",
    "                        target_indices = []\n",
    "                        for idx in target:\n",
    "                            token = idx.item()\n",
    "                            if token == tokenizer.chars.index('EOS'):\n",
    "                                break\n",
    "                            if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n",
    "                                target_indices.append(token)\n",
    "                        \n",
    "                        # Decode target\n",
    "                        target_text = tokenizer.decode(target_indices)\n",
    "                        target_text = target_text.replace('SOS', '').replace('EOS', '')\n",
    "                        current_chunk['gts'].append(target_text)\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del predictions, trg\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # If chunk is full or this is the last batch, append to main lists and clear chunk\n",
    "                if len(current_chunk['imgs']) >= chunk_size or batch_idx == len(test_loader) - 1:\n",
    "                    imgs.extend(current_chunk['imgs'])\n",
    "                    predicts.extend(current_chunk['preds'])\n",
    "                    gt.extend(current_chunk['gts'])\n",
    "                    \n",
    "                    # Clear chunk\n",
    "                    current_chunk = {'imgs': [], 'preds': [], 'gts': []}\n",
    "                    \n",
    "                    # Force garbage collection\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error during testing: {e}\")\n",
    "            # Save what we have so far\n",
    "            if current_chunk['imgs']:\n",
    "                imgs.extend(current_chunk['imgs'])\n",
    "                predicts.extend(current_chunk['preds'])\n",
    "                gt.extend(current_chunk['gts'])\n",
    "    \n",
    "    return predicts, gt, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:53.935412Z",
     "iopub.status.busy": "2025-06-04T17:50:53.935153Z",
     "iopub.status.idle": "2025-06-04T17:50:56.428538Z",
     "shell.execute_reply": "2025-06-04T17:50:56.427984Z",
     "shell.execute_reply.started": "2025-06-04T17:50:53.935389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear any existing cached memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Configure DataLoader for minimal memory usage\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    DataGenerator(source_path, charset_base, max_text_length, 'test', transform), \n",
    "    batch_size=1,  # Keep batch size at 1 for minimum memory usage\n",
    "    shuffle=False, \n",
    "    num_workers=0,  # Single process loading\n",
    "    pin_memory=False,  # Disable pin_memory to reduce memory usage\n",
    "    persistent_workers=False,  # Disable persistent workers\n",
    "    prefetch_factor=None  # Disable prefetching\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:50:56.429765Z",
     "iopub.status.busy": "2025-06-04T17:50:56.429492Z",
     "iopub.status.idle": "2025-06-04T17:52:54.284725Z",
     "shell.execute_reply": "2025-06-04T17:52:54.283987Z",
     "shell.execute_reply.started": "2025-06-04T17:50:56.429741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predicts, gt, imgs = test(model, test_loader, max_text_length, tokenizer)\n",
    "\n",
    "\n",
    "# the part below is causing error \n",
    "predicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\n",
    "gt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:52:54.285815Z",
     "iopub.status.busy": "2025-06-04T17:52:54.285573Z",
     "iopub.status.idle": "2025-06-04T17:52:54.300882Z",
     "shell.execute_reply": "2025-06-04T17:52:54.300091Z",
     "shell.execute_reply.started": "2025-06-04T17:52:54.285792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Character Error Rate 1.0, Word Error Rate 1.0 and Sequence Error Rate 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate = evaluation.ocr_metrics(predicts=predicts,\n",
    "                                  ground_truth=gt,)\n",
    " \n",
    "print(\"Calculate Character Error Rate {}, Word Error Rate {} and Sequence Error Rate {}\".format(evaluate[0],evaluate[1],evaluate[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T17:52:54.302917Z",
     "iopub.status.busy": "2025-06-04T17:52:54.302719Z",
     "iopub.status.idle": "2025-06-04T17:52:54.320891Z",
     "shell.execute_reply": "2025-06-04T17:52:54.320204Z",
     "shell.execute_reply.started": "2025-06-04T17:52:54.302902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from data import preproc as pp\n",
    "\n",
    "def show_predictions(imgs, gt, predicts, max_display=10):\n",
    "    \"\"\"\n",
    "    Display images with their ground truth and predicted text.\n",
    "\n",
    "    Args:\n",
    "        imgs: List of image tensors (C, H, W).\n",
    "        gt: List of ground truth strings.\n",
    "        predicts: List of predicted strings.\n",
    "        max_display: Maximum number of images to display.\n",
    "    \"\"\"\n",
    "    for i, item in enumerate(imgs[:max_display]):\n",
    "        print(\"=\" * 80)\n",
    "        img = item.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        # Convert to grayscale if image has 3 channels\n",
    "        if img.shape[2] == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = pp.adjust_to_see(img)\n",
    "        cv2.imshow('Line', img)\n",
    "        print(\"Ground truth:\", gt[i])\n",
    "        print(\"Prediction :\", predicts[i], \"\\n\")\n",
    "        cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-04T17:52:59.214Z",
     "iopub.execute_input": "2025-06-04T17:52:54.321774Z",
     "iopub.status.busy": "2025-06-04T17:52:54.321599Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# from src.utils.display_results import show_predictions\n",
    "\n",
    "# Assuming you have run the test function and obtained these:\n",
    "# predicts, gt, imgs = test(model, test_loader, max_text_length, tokenizer)\n",
    "\n",
    "# Call the display function to show images with predictions and ground truth\n",
    "show_predictions(imgs, gt, predicts, max_display=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T20:55:43.083771Z",
     "iopub.status.idle": "2025-05-25T20:55:43.084113Z",
     "shell.execute_reply": "2025-05-25T20:55:43.083959Z",
     "shell.execute_reply.started": "2025-05-25T20:55:43.083945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-25T20:55:43.085124Z",
     "iopub.status.idle": "2025-05-25T20:55:43.085470Z",
     "shell.execute_reply": "2025-05-25T20:55:43.085319Z",
     "shell.execute_reply.started": "2025-05-25T20:55:43.085303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "training_results = joblib.load(output_path + '/training_results.joblib')\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "# Extract data\n",
    "train_losses = [r[0] for r in training_results]\n",
    "valid_losses = [r[1] for r in training_results]\n",
    "durations = [r[2] for r in training_results]\n",
    "epoch_nums = list(range(1, len(training_results) + 1))\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Training Loss\n",
    "sns.lineplot(x=epoch_nums, y=train_losses, marker='o', ax=axs[0], color='blue')\n",
    "axs[0].set_title('Training Loss per Epoch')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Training Loss')\n",
    "\n",
    "# Validation Loss\n",
    "sns.lineplot(x=epoch_nums, y=valid_losses, marker='o', ax=axs[1], color='green')\n",
    "axs[1].set_title('Validation Loss per Epoch')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Validation Loss')\n",
    "\n",
    "# Time per Epoch\n",
    "sns.lineplot(x=epoch_nums, y=durations, marker='o', ax=axs[2], color='red')\n",
    "axs[2].set_title('Time per Epoch')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].set_ylabel('Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7249797,
     "sourceId": 11562582,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
