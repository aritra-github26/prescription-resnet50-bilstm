{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11562582,"sourceType":"datasetVersion","datasetId":7249797}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:49:24.697283Z","iopub.execute_input":"2025-06-04T17:49:24.697730Z","iopub.status.idle":"2025-06-04T17:49:24.721692Z","shell.execute_reply.started":"2025-06-04T17:49:24.697702Z","shell.execute_reply":"2025-06-04T17:49:24.720918Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/handwriting-hdf5/data/washington.hdf5\n/kaggle/input/handwriting-hdf5/data/bentham.hdf5\n/kaggle/input/handwriting-hdf5/data/iam.hdf5\n/kaggle/input/handwriting-hdf5/data/saintgall.hdf5\n/kaggle/input/handwriting-hdf5/data/merged/merged.hdf5\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"if not os.path.exists('/kaggle/working/experimental-transformer-ocr'):\n    ! git clone https://github.com/aritra-github26/experimental-transformer-ocr.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:49:27.659382Z","iopub.execute_input":"2025-06-04T17:49:27.659631Z","iopub.status.idle":"2025-06-04T17:49:27.663686Z","shell.execute_reply.started":"2025-06-04T17:49:27.659611Z","shell.execute_reply":"2025-06-04T17:49:27.662987Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.chdir('/kaggle/working/experimental-transformer-ocr')\n! pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:49:28.599302Z","iopub.execute_input":"2025-06-04T17:49:28.599557Z","iopub.status.idle":"2025-06-04T17:49:28.718764Z","shell.execute_reply.started":"2025-06-04T17:49:28.599537Z","shell.execute_reply":"2025-06-04T17:49:28.717897Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/experimental-transformer-ocr\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:49:30.649617Z","iopub.execute_input":"2025-06-04T17:49:30.650262Z","iopub.status.idle":"2025-06-04T17:50:44.642482Z","shell.execute_reply.started":"2025-06-04T17:49:30.650233Z","shell.execute_reply":"2025-06-04T17:50:44.641792Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.60.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.11.0.86)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.21.0+cu124)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (3.13.0)\nRequirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.8.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->-r requirements.txt (line 2)) (2.4.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->-r requirements.txt (line 4)) (0.43.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 10))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 10))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 10))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 10))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 10))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 10))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 10))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 10)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 10)) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 12)) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 10)) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r requirements.txt (line 2)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->-r requirements.txt (line 2)) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->-r requirements.txt (line 2)) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"os.chdir('/kaggle/working/experimental-transformer-ocr/src')\n! pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:44.643710Z","iopub.execute_input":"2025-06-04T17:50:44.644027Z","iopub.status.idle":"2025-06-04T17:50:44.760734Z","shell.execute_reply.started":"2025-06-04T17:50:44.644000Z","shell.execute_reply":"2025-06-04T17:50:44.760067Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/experimental-transformer-ocr/src\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport math\nfrom itertools import groupby\nimport h5py\nimport numpy as np\nimport unicodedata\nimport cv2\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet50, resnet34\nfrom torch.autograd import Variable\nimport torchvision\nfrom data import preproc as pp\nfrom data import evaluation\nfrom torch.utils.data import Dataset\nimport time\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:44.762086Z","iopub.execute_input":"2025-06-04T17:50:44.762379Z","iopub.status.idle":"2025-06-04T17:50:52.200823Z","shell.execute_reply.started":"2025-06-04T17:50:44.762336Z","shell.execute_reply":"2025-06-04T17:50:52.200005Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=128):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass OCR(nn.Module):\n\n    def __init__(self, vocab_len, hidden_dim):\n        super().__init__()\n\n        # create ResNet-50 backbone\n        self.backbone = resnet50()\n        del self.backbone.fc\n\n        # create conversion layer\n        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n\n        # create a BiLSTM layer\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, bidirectional=True, batch_first=True)\n\n        # prediction heads with length of vocab\n        self.vocab = nn.Linear(hidden_dim * 2, vocab_len)  # Adjusted for bidirectional output\n\n        # spatial positional encodings\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.query_pos = PositionalEncoding(hidden_dim, .2)\n\n    def get_feature(self, x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)   \n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n        return x\n\n    def forward(self, inputs):  # Remove trg parameter\n        \"\"\"\n        Forward pass through the model.\n        \n        Args:\n            inputs: Input images (batch of images).\n        \n        Returns:\n            Output predictions for the sequences.\n        \"\"\"\n\n        # Propagate inputs through ResNet-50\n        x = self.get_feature(inputs)\n\n        # Convert from 2048 to hidden_dim feature planes\n        h = self.conv(x)  # shape: (batch, hidden_dim, H, W)\n\n        # Add spatial positional encodings\n        h_shape = h.shape\n        H, W = h_shape[2], h_shape[3]\n        # Expand row and col embeddings to match spatial dimensions\n        row_emb = self.row_embed[:H].unsqueeze(1).repeat(1, W, 1)  # (H, W, hidden_dim//2)\n        col_emb = self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1)  # (H, W, hidden_dim//2)\n        pos_emb = torch.cat([row_emb, col_emb], dim=-1).permute(2, 0, 1).unsqueeze(0)  # (1, hidden_dim, H, W)\n        pos_emb = pos_emb.to(h.device)\n        h = h + pos_emb  # Add positional encoding to feature map\n\n        # Prepare input for LSTM: flatten spatial dims and permute to (batch, seq_len, feature)\n        h = h.flatten(2).permute(0, 2, 1)  # (batch, seq_len, feature)\n\n        # Add positional encoding to LSTM input features\n        h = h.permute(1, 0, 2)  # (seq_len, batch, feature)\n        h = self.query_pos(h)\n        h = h.permute(1, 0, 2)  # (batch, seq_len, feature)\n\n        # Pass through BiLSTM\n        h, _ = self.lstm(h)  # output shape: (batch, seq_len, hidden_dim*2)\n\n        # Permute output to (seq_len, batch, feature)\n        h = h.permute(1, 0, 2)\n\n        # Calculate output\n        output = self.vocab(h)  # (seq_len, batch, vocab_size)\n\n        return output\n\n\ndef make_model(vocab_len, hidden_dim=256):\n    \n    return OCR(vocab_len, hidden_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:52.202748Z","iopub.execute_input":"2025-06-04T17:50:52.203140Z","iopub.status.idle":"2025-06-04T17:50:52.214659Z","shell.execute_reply.started":"2025-06-04T17:50:52.203121Z","shell.execute_reply":"2025-06-04T17:50:52.214087Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\"\"\"\nUses generator functions to supply train/test with data.\nImage renderings and text are created on the fly each time.\n\"\"\"\n\nclass DataGenerator(Dataset):\n    \"\"\"Generator class with data streaming\"\"\"\n\n    def __init__(self, source,charset, max_text_length, split, transform):\n        self.tokenizer = Tokenizer(charset, max_text_length)\n        self.transform = transform\n        \n        self.split = split\n        self.dataset = dict()\n\n        with h5py.File(source, \"r\") as f:\n            self.dataset[self.split] = dict()\n\n            self.dataset[self.split]['dt'] = np.array(f[self.split]['dt'])\n            self.dataset[self.split]['gt'] = np.array(f[self.split]['gt'])\n          \n            randomize = np.arange(len(self.dataset[self.split]['gt']))\n            np.random.seed(42)\n            np.random.shuffle(randomize)\n\n            self.dataset[self.split]['dt'] = self.dataset[self.split]['dt'][randomize]\n            self.dataset[self.split]['gt'] = self.dataset[self.split]['gt'][randomize]\n\n            # decode sentences from byte\n            self.dataset[self.split]['gt'] = [x.decode() for x in self.dataset[self.split]['gt']]\n            \n        self.size = len(self.dataset[self.split]['gt'])\n\n\n    def __getitem__(self, i):\n        img = self.dataset[self.split]['dt'][i]\n        \n        #making image compatible with resnet\n        img = np.repeat(img[..., np.newaxis],3, -1)    \n        img = pp.normalization(img)\n        \n        if self.transform is not None:\n            img = self.transform(img)\n\n        y_train = self.tokenizer.encode(self.dataset[self.split]['gt'][i]) \n        \n        #padding till max length\n        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)))\n\n        gt = torch.Tensor(y_train)\n\n        return img, gt          \n\n    def __len__(self):\n      return self.size\n\n\n\nclass Tokenizer():\n    \"\"\"Manager tokens functions and charset/dictionary properties\"\"\"\n\n    def __init__(self, chars, max_text_length=128):\n        self.PAD_TK, self.UNK_TK,self.SOS,self.EOS = \"¶\", \"¤\", \"SOS\", \"EOS\"\n        self.chars = [self.PAD_TK] + [self.UNK_TK ]+ [self.SOS] + [self.EOS] +list(chars)\n        self.PAD = self.chars.index(self.PAD_TK)\n        self.UNK = self.chars.index(self.UNK_TK)\n\n        self.vocab_size = len(self.chars)\n        self.maxlen = max_text_length\n\n    def encode(self, text):\n        \"\"\"Encode text to vector\"\"\"\n\n        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n        text = \" \".join(text.split())\n\n        groups = [\"\".join(group) for _, group in groupby(text)]\n        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n        encoded = []\n\n        text = ['SOS'] + list(text) + ['EOS']\n        for item in text:\n            index = self.chars.index(item)\n            index = self.UNK if index == -1 else index\n            encoded.append(index)\n\n        return np.asarray(encoded)\n\n    def decode(self, text):\n        \"\"\"Decode vector to text\"\"\"\n        \n        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n        decoded = self.remove_tokens(decoded)\n        decoded = pp.text_standardize(decoded)\n\n        return decoded\n\n    def remove_tokens(self, text):\n        \"\"\"Remove tokens (PAD) from text\"\"\"\n\n        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:52.215273Z","iopub.execute_input":"2025-06-04T17:50:52.215447Z","iopub.status.idle":"2025-06-04T17:50:52.972646Z","shell.execute_reply.started":"2025-06-04T17:50:52.215432Z","shell.execute_reply":"2025-06-04T17:50:52.971974Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nimport datetime\nimport string\n\nbatch_size = 16\nepochs = 200\n\n# define paths\n#change paths accordingly\n\nsource_path = '/kaggle/input/handwriting-hdf5/data/merged/merged.hdf5'\noutput_path = '/kaggle/working/output'\ntarget_path = output_path + '/merged_training_weights_ctc_150.pt'\n\nos.makedirs(output_path, exist_ok=True)\n\n# define input size, number max of chars per line and list of valid chars\ninput_size = (1024, 128, 1)\nmax_text_length = 128\ncharset_base = string.printable[:95]\n\nprint(\"source:\", source_path)\nprint(\"output:\", output_path)\nprint(\"target\", target_path)\nprint(\"charset:\", charset_base)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:52.973341Z","iopub.execute_input":"2025-06-04T17:50:52.973532Z","iopub.status.idle":"2025-06-04T17:50:52.989814Z","shell.execute_reply.started":"2025-06-04T17:50:52.973516Z","shell.execute_reply":"2025-06-04T17:50:52.989223Z"}},"outputs":[{"name":"stdout","text":"source: /kaggle/input/handwriting-hdf5/data/merged/merged.hdf5\noutput: /kaggle/working/output\ntarget /kaggle/working/output/merged_training_weights_ctc_150.pt\ncharset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torchvision.transforms as T\n\ndevice = torch.device(\"cuda\")\ntransform = T.Compose([\n    T.ToTensor()])\ntokenizer = Tokenizer(charset_base)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:52.990543Z","iopub.execute_input":"2025-06-04T17:50:52.990779Z","iopub.status.idle":"2025-06-04T17:50:53.003856Z","shell.execute_reply.started":"2025-06-04T17:50:52.990758Z","shell.execute_reply":"2025-06-04T17:50:53.003356Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\n\ntrain_loader = torch.utils.data.DataLoader(DataGenerator(source_path,charset_base,max_text_length,'train',transform), batch_size=batch_size, shuffle=False, num_workers=2)\nval_loader = torch.utils.data.DataLoader(DataGenerator(source_path,charset_base,max_text_length,'valid',transform), batch_size=batch_size, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T16:27:18.898809Z","iopub.execute_input":"2025-06-04T16:27:18.899063Z","iopub.status.idle":"2025-06-04T16:27:33.797317Z","shell.execute_reply.started":"2025-06-04T16:27:18.899038Z","shell.execute_reply":"2025-06-04T16:27:33.796743Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model = make_model(vocab_len=tokenizer.vocab_size)\n_=model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T16:50:10.451533Z","iopub.execute_input":"2025-06-04T16:50:10.451788Z","iopub.status.idle":"2025-06-04T16:50:10.494052Z","shell.execute_reply.started":"2025-06-04T16:50:10.451771Z","shell.execute_reply":"2025-06-04T16:50:10.493173Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1703023475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"criterion = nn.CTCLoss(blank=0, zero_infinity=True)\ncriterion.to(device)\nlr = .0001 # learning rate\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr,weight_decay=.0004)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T16:27:34.516804Z","iopub.execute_input":"2025-06-04T16:27:34.517078Z","iopub.status.idle":"2025-06-04T16:27:34.522354Z","shell.execute_reply.started":"2025-06-04T16:27:34.517059Z","shell.execute_reply":"2025-06-04T16:27:34.521475Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n\ndef train(model, criterion, optimizer, scheduler, dataloader, vocab_length, device):\n    \"\"\"\n    Train the model using the provided dataloader.\n    \n    Args:\n        model: The OCR model\n        criterion: Loss function (CTC)\n        optimizer: Optimizer instance\n        scheduler: Learning rate scheduler\n        dataloader: Training data loader\n        vocab_length: Size of vocabulary\n        device: Device to train on\n        \n    Returns:\n        float: Average loss for the epoch\n    \"\"\"\n    model.train()\n    total_loss = 0\n    total_items = 0\n    \n    for batch, (imgs, labels_y,) in enumerate(dataloader):\n        imgs = imgs.to(device)\n        labels_y = labels_y.to(device)\n        batch_size = imgs.size(0)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(imgs.float())\n        \n        # Ensure output is in (batch, seq_len, vocab_size) format\n        if output.dim() != 3:\n            raise ValueError(f\"Expected 3D output tensor, got shape: {output.shape}\")\n        \n        # Permute to (seq_len, batch, vocab_size) for CTC loss\n        log_probs = F.log_softmax(output, dim=2).permute(1, 0, 2)\n        \n        # Calculate input sequence lengths (all are same length after CNN processing)\n        input_lengths = torch.full((batch_size,), \n                                 log_probs.size(0), \n                                 dtype=torch.long,\n                                 device=device)\n        \n        # Calculate target lengths (excluding padding)\n        target_lengths = []\n        labels_list = []\n        valid_samples = []\n        \n        # Process each sequence in the batch\n        for i in range(batch_size):\n            # Find non-zero elements (non-padding)\n            non_zero = labels_y[i].nonzero().squeeze()\n            if non_zero.dim() == 0:  # Handle case of empty sequence\n                continue  # Skip this sample\n            \n            length = non_zero.shape[0]\n            if length > log_probs.size(0):  # Skip if target is longer than output\n                continue\n                \n            sequence = labels_y[i, :length]\n            target_lengths.append(length)\n            labels_list.append(sequence)\n            valid_samples.append(i)\n        \n        # Skip batch if no valid samples\n        if not valid_samples:\n            continue\n            \n        # Keep only valid samples\n        valid_samples = torch.tensor(valid_samples, device=device)\n        log_probs = log_probs[:, valid_samples, :]\n        input_lengths = input_lengths[valid_samples]\n        \n        # Convert target lengths to tensor\n        target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n        \n        # Concatenate all label sequences\n        labels_packed = torch.cat(labels_list)\n        \n        # CTC loss calculation\n        try:\n            loss = criterion(log_probs,  # (T, N, C)\n                           labels_packed,  # Flattened target sequences\n                           input_lengths,  # Length of each input sequence (N,)\n                           target_lengths)  # Length of each target sequence (N,)\n            \n            loss.backward()\n            \n            # Gradient clipping to prevent explosion\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            total_loss += loss.item() * len(valid_samples)\n            total_items += len(valid_samples)\n            \n        except RuntimeError as e:\n            print(f\"Error in batch {batch}:\")\n            print(f\"log_probs shape: {log_probs.shape}\")\n            print(f\"labels_packed shape: {labels_packed.shape}\")\n            print(f\"input_lengths shape: {input_lengths.shape}\")\n            print(f\"target_lengths shape: {target_lengths.shape}\")\n            raise e\n    \n    return total_loss / total_items if total_items > 0 else float('inf')\n\ndef evaluate(model, criterion, dataloader, vocab_length, device):\n    \"\"\"\n    Evaluate the model using the provided dataloader.\n    \n    Args:\n        model: The OCR model\n        criterion: Loss function (CTC)\n        dataloader: Validation data loader\n        vocab_length: Size of vocabulary\n        device: Device to evaluate on\n        \n    Returns:\n        float: Average loss for the epoch\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    total_items = 0\n\n    with torch.no_grad():\n        for batch, (imgs, labels_y) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            labels_y = labels_y.to(device)\n            batch_size = imgs.size(0)\n\n            # Forward pass\n            output = model(imgs.float())\n            \n            # Ensure output is in (batch, seq_len, vocab_size) format\n            if output.dim() != 3:\n                raise ValueError(f\"Expected 3D output tensor, got shape: {output.shape}\")\n            \n            # Permute to (seq_len, batch, vocab_size) for CTC loss\n            log_probs = F.log_softmax(output, dim=2).permute(1, 0, 2)\n            \n            # Calculate input sequence lengths (all are same length after CNN processing)\n            input_lengths = torch.full((batch_size,), \n                                     log_probs.size(0), \n                                     dtype=torch.long,\n                                     device=device)\n            \n            # Calculate target lengths (excluding padding)\n            target_lengths = []\n            labels_list = []\n            valid_samples = []\n            \n            # Process each sequence in the batch\n            for i in range(batch_size):\n                # Find non-zero elements (non-padding)\n                non_zero = labels_y[i].nonzero().squeeze()\n                if non_zero.dim() == 0:  # Handle case of empty sequence\n                    continue  # Skip this sample\n                \n                length = non_zero.shape[0]\n                if length > log_probs.size(0):  # Skip if target is longer than output\n                    continue\n                    \n                sequence = labels_y[i, :length]\n                target_lengths.append(length)\n                labels_list.append(sequence)\n                valid_samples.append(i)\n            \n            # Skip batch if no valid samples\n            if not valid_samples:\n                continue\n                \n            # Keep only valid samples\n            valid_samples = torch.tensor(valid_samples, device=device)\n            log_probs = log_probs[:, valid_samples, :]\n            input_lengths = input_lengths[valid_samples]\n            \n            # Convert target lengths to tensor\n            target_lengths = torch.tensor(target_lengths, dtype=torch.long, device=device)\n            \n            # Concatenate all label sequences\n            labels_packed = torch.cat(labels_list)\n            \n            try:\n                # CTC loss calculation\n                loss = criterion(log_probs,\n                               labels_packed,\n                               input_lengths,\n                               target_lengths)\n                \n                total_loss += loss.item() * len(valid_samples)\n                total_items += len(valid_samples)\n                \n            except RuntimeError as e:\n                print(f\"Error in batch {batch}:\")\n                print(f\"log_probs shape: {log_probs.shape}\")\n                print(f\"labels_packed shape: {labels_packed.shape}\")\n                print(f\"input_lengths shape: {input_lengths.shape}\")\n                print(f\"target_lengths shape: {target_lengths.shape}\")\n                raise e\n\n    return total_loss / total_items if total_items > 0 else float('inf')\n\ndef get_memory(model, imgs):\n    \"\"\"\n    Extract features and apply positional encoding for the BiLSTM model.\n    \n    Args:\n        model: The OCR model\n        imgs: Input images tensor\n        \n    Returns:\n        Memory tensor with shape (seq_len, batch, hidden_dim*2)\n    \"\"\"\n    with torch.no_grad():\n        # Extract CNN features\n        features = model.get_feature(imgs)\n        \n        # Apply conv layer\n        conv_out = model.conv(features)\n        \n        # Get spatial dimensions\n        bs, c, h, w = conv_out.size()\n        \n        # Add positional encodings\n        row_emb = model.row_embed[:h].unsqueeze(1).repeat(1, w, 1)  # (H, W, hidden_dim//2)\n        col_emb = model.col_embed[:w].unsqueeze(0).repeat(h, 1, 1)  # (H, W, hidden_dim//2)\n        pos_emb = torch.cat([row_emb, col_emb], dim=-1).permute(2, 0, 1).unsqueeze(0)  # (1, hidden_dim, H, W)\n        pos_emb = pos_emb.to(conv_out.device)\n        conv_out = conv_out + pos_emb\n        \n        # Flatten spatial dimensions and permute for LSTM\n        lstm_input = conv_out.flatten(2).permute(0, 2, 1)  # (batch, seq_len, feature)\n        \n        # Add positional encoding to LSTM input\n        lstm_input = lstm_input.permute(1, 0, 2)  # (seq_len, batch, feature)\n        lstm_input = model.query_pos(lstm_input)\n        lstm_input = lstm_input.permute(1, 0, 2)  # (batch, seq_len, feature)\n        \n        # Apply BiLSTM\n        lstm_out, _ = model.lstm(lstm_input)\n        \n        # Return in shape (seq_len, batch, hidden_dim*2)\n        return lstm_out.permute(1, 0, 2)\n\ndef single_image_inference(model, img, tokenizer, transform, device):\n    \"\"\"\n    Run inference on single image using greedy decoding.\n    \n    Args:\n        model: The OCR model\n        img: Input image\n        tokenizer: Tokenizer for encoding/decoding text\n        transform: Image transform pipeline\n        device: Device to run inference on\n        \n    Returns:\n        pred_text: Predicted text string\n    \"\"\"\n    model.eval()\n    \n    # Preprocess image\n    img = transform(img)\n    imgs = img.unsqueeze(0).float().to(device)\n    \n    with torch.no_grad():\n        # Forward pass\n        output = model(imgs)\n        \n        # Ensure output is in (seq_len, batch, vocab_size) format\n        if output.dim() == 3:\n            output = output.permute(1, 0, 2)\n        \n        # Apply log softmax and get predictions\n        output = F.log_softmax(output, dim=2)\n        output = output.argmax(dim=2)\n        output = output.squeeze(1)  # Remove batch dimension\n        \n        # Convert prediction to text (handle special tokens)\n        out_indices = []\n        for idx in output:\n            token = idx.item()\n            if token == tokenizer.chars.index('EOS'):\n                break\n            if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n                out_indices.append(token)\n        \n        # Decode the prediction\n        pred_text = tokenizer.decode(out_indices)\n    \n    return pred_text\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\ndef run_epochs(model, criterion, optimizer, scheduler, train_loader, val_loader, epochs, tokenizer, target_path, device):\n    \"\"\"\n    Run training for specified number of epochs.\n    \n    Args:\n        model: The OCR model\n        criterion: Loss function (CTC)\n        optimizer: Optimizer instance\n        scheduler: Learning rate scheduler\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        epochs: Number of epochs to train\n        tokenizer: Tokenizer for encoding/decoding text\n        target_path: Path to save model checkpoints\n        device: Device to train on\n    \"\"\"\n    best_valid_loss = float('inf')\n    patience = 0\n    max_patience = 4  # Number of epochs to wait before reducing learning rate\n    \n    for epoch in range(epochs):\n        print(f'Epoch: {epoch + 1:02} | Learning rate: {scheduler.get_last_lr()[0]:.6f}')\n        \n        start_time = time.time()\n        \n        # Training phase\n        train_loss = train(model, criterion, optimizer, scheduler, \n                          train_loader, tokenizer.vocab_size, device)\n        \n        # Validation phase\n        valid_loss = evaluate(model, criterion, val_loader, tokenizer.vocab_size, device)\n        \n        epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n        \n        # Save best model based on validation loss\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'best_valid_loss': best_valid_loss,\n            }, target_path)\n            patience = 0\n        else:\n            patience += 1\n        \n        # Reduce learning rate if validation loss hasn't improved\n        if patience >= max_patience:\n            scheduler.step()\n            patience = 0\n        \n        print(f'Time: {epoch_mins}m {epoch_secs}s')\n        print(f'Train Loss: {train_loss:.3f}')\n        print(f'Val   Loss: {valid_loss:.3f}')\n        print(f'Best Val Loss: {best_valid_loss:.3f}')\n    \n    print(f'Training completed. Best validation loss: {best_valid_loss:.3f}')\n\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:53.004537Z","iopub.execute_input":"2025-06-04T17:50:53.004718Z","iopub.status.idle":"2025-06-04T17:50:53.030576Z","shell.execute_reply.started":"2025-06-04T17:50:53.004704Z","shell.execute_reply":"2025-06-04T17:50:53.029860Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# train model\n# This is the actual code :)\n\n\n\nimport joblib\ntraining_results = [] # format: [training loss, validation loss, epoch time in seconds]\n \ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\n# def run_epochs(model, criterion, optimizer, scheduler, train_loader, val_loader, epochs, tokenizer, target_path, device):\n'''\nrun one epoch for a model\n'''\nepochs = 3\nc = 0\nbest_valid_loss = float('inf')\n\nfor epoch in range(epochs):     \n    print(f'Epoch: {epoch + 1:02}', 'learning rate{}'.format(scheduler.get_last_lr()))\n    \n    start_time = time.time()\n    \n    # Training phase\n    train_loss = train(model, criterion, optimizer, scheduler, \n                              train_loader, tokenizer.vocab_size, device)\n    \n    # Validation phase\n    valid_loss = evaluate(model, criterion, val_loader, tokenizer.vocab_size, device)\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n    \n    # Save best model based on validation loss\n    c += 1\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        end_time = time.time()\n        torch.save(model.state_dict(), target_path)\n        training_results.append((train_loss, valid_loss, int(end_time - start_time)))\n        c = 0\n    \n    if c > 4:\n        scheduler.step()\n        c = 0\n    \n    \n    print(f'Time: {epoch_mins}m {epoch_secs}s')\n    print(f'Train Loss: {train_loss:.3f}')\n    print(f'Val   Loss: {valid_loss:.3f}')\n\nprint(best_valid_loss)\n\n\n# training_results.append((train_loss, valid_loss, int(end_time - start_time)))\njoblib.dump(training_results, output_path + '/training_results.joblib')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:44:08.869251Z","iopub.execute_input":"2025-05-25T20:44:08.869456Z","iopub.status.idle":"2025-05-25T20:55:21.080851Z","shell.execute_reply.started":"2025-05-25T20:44:08.869433Z","shell.execute_reply":"2025-05-25T20:55:21.080157Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01 learning rate[0.0001]\nTime: 3m 44s\nTrain Loss: 3.726\nVal   Loss: 3.645\nEpoch: 02 learning rate[0.0001]\nTime: 3m 42s\nTrain Loss: 3.612\nVal   Loss: 3.650\nEpoch: 03 learning rate[0.0001]\nTime: 3m 44s\nTrain Loss: 3.601\nVal   Loss: 3.648\n3.6454115870972754\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/output/training_results.joblib']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model = make_model(vocab_len=tokenizer.vocab_size)\n_=model.to(device)\n\n# model.load_state_dict(torch.load(target_path))\nmodel.load_state_dict(torch.load(target_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:53.031280Z","iopub.execute_input":"2025-06-04T17:50:53.031494Z","iopub.status.idle":"2025-06-04T17:50:53.891299Z","shell.execute_reply.started":"2025-06-04T17:50:53.031478Z","shell.execute_reply":"2025-06-04T17:50:53.890663Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# def get_memory(model, imgs):\n#     # Refactored get_memory for resnet50-biLSTM model: simply extract features and apply conv and lstm\n#     with torch.no_grad():\n#         features = model.get_feature(imgs)\n#         conv_out = model.conv(features)\n#         bs, c, h, w = conv_out.size()\n#         # Flatten spatial dimensions and permute for LSTM input: (batch, seq_len, feature)\n#         lstm_input = conv_out.flatten(2).permute(0, 2, 1)\n#         lstm_out, _ = model.lstm(lstm_input)\n#     return lstm_out.permute(1, 0, 2)  # Return in shape (seq_len, batch, feature) for compatibility\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:53.893387Z","iopub.execute_input":"2025-06-04T17:50:53.893604Z","iopub.status.idle":"2025-06-04T17:50:53.897097Z","shell.execute_reply.started":"2025-06-04T17:50:53.893588Z","shell.execute_reply":"2025-06-04T17:50:53.896384Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# def test(model, test_loader, max_text_length, tokenizer):\n#     \"\"\"\n#     Evaluate and predict model with the test dataloader.\n    \n#     Args:\n#         model: The OCR model to evaluate.\n#         test_loader: DataLoader for test dataset.\n#         max_text_length: Maximum length of output sequence.\n#         tokenizer: Tokenizer for decoding output tokens.\n    \n#     Returns:\n#         predicts: List of predicted text sequences.\n#         gt: List of ground truth text sequences.\n#         imgs: List of input images.\n#     \"\"\"\n#     model.eval()\n#     predicts = []\n#     gt = []\n#     imgs = []\n#     device = next(model.parameters()).device\n    \n#     with torch.no_grad():\n#         for batch in test_loader:\n#             src, trg = batch\n#             imgs.append(src.flatten(0,1))\n#             src = src.to(device)\n#             trg = trg.to(device)\n            \n#             # Forward pass without teacher forcing\n#             output = model(src.float())\n            \n#             # Ensure output is in (seq_len, batch, vocab_size) format\n#             if output.dim() == 3:\n#                 output = output.permute(1, 0, 2)\n            \n#             # Apply log softmax and get predictions\n#             output = F.log_softmax(output, dim=2)\n#             predictions = output.argmax(dim=2)\n            \n#             # Process each sequence in the batch\n#             for pred, target in zip(predictions.transpose(0,1), trg):\n#                 # Convert prediction to text (handle special tokens)\n#                 pred_indices = []\n#                 for idx in pred:\n#                     token = idx.item()\n#                     if token == tokenizer.chars.index('EOS'):\n#                         break\n#                     if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n#                         pred_indices.append(token)\n                \n#                 # Decode prediction\n#                 pred_text = tokenizer.decode(pred_indices)\n#                 pred_text = pred_text.replace('SOS', '').replace('EOS', '')\n#                 predicts.append(pred_text)\n                \n#                 # Convert target to text (handle special tokens)\n#                 target_indices = []\n#                 for idx in target:\n#                     token = idx.item()\n#                     if token == tokenizer.chars.index('EOS'):\n#                         break\n#                     if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n#                         target_indices.append(token)\n                \n#                 # Decode target\n#                 target_text = tokenizer.decode(target_indices)\n#                 target_text = target_text.replace('SOS', '').replace('EOS', '')\n#                 gt.append(target_text)\n    \n#     return predicts, gt, imgs\n\n\ndef calculate_cer(pred_text, target_text):\n    \"\"\"Calculate Character Error Rate using Levenshtein distance.\"\"\"\n    if len(target_text) == 0:\n        return 0 if len(pred_text) == 0 else 1\n        \n    matrix = [[0 for _ in range(len(pred_text) + 1)] \n              for _ in range(len(target_text) + 1)]\n    \n    for i in range(len(target_text) + 1):\n        matrix[i][0] = i\n    for j in range(len(pred_text) + 1):\n        matrix[0][j] = j\n        \n    for i in range(1, len(target_text) + 1):\n        for j in range(1, len(pred_text) + 1):\n            if target_text[i-1] == pred_text[j-1]:\n                matrix[i][j] = matrix[i-1][j-1]\n            else:\n                matrix[i][j] = min(matrix[i-1][j-1] + 1,    # substitution\n                                 matrix[i][j-1] + 1,         # insertion\n                                 matrix[i-1][j] + 1)         # deletion\n                \n    return matrix[len(target_text)][len(pred_text)] / len(target_text)\n\ndef calculate_wer(pred_text, target_text):\n    \"\"\"Calculate Word Error Rate using word-level Levenshtein distance.\"\"\"\n    pred_words = pred_text.split()\n    target_words = target_text.split()\n    \n    if len(target_words) == 0:\n        return 0 if len(pred_words) == 0 else 1\n        \n    matrix = [[0 for _ in range(len(pred_words) + 1)] \n              for _ in range(len(target_words) + 1)]\n    \n    for i in range(len(target_words) + 1):\n        matrix[i][0] = i\n    for j in range(len(pred_words) + 1):\n        matrix[0][j] = j\n        \n    for i in range(1, len(target_words) + 1):\n        for j in range(1, len(pred_words) + 1):\n            if target_words[i-1] == pred_words[j-1]:\n                matrix[i][j] = matrix[i-1][j-1]\n            else:\n                matrix[i][j] = min(matrix[i-1][j-1] + 1,    # substitution\n                                 matrix[i][j-1] + 1,         # insertion\n                                 matrix[i-1][j] + 1)         # deletion\n                \n    return matrix[len(target_words)][len(pred_words)] / len(target_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:53.897903Z","iopub.execute_input":"2025-06-04T17:50:53.898141Z","iopub.status.idle":"2025-06-04T17:50:53.915308Z","shell.execute_reply.started":"2025-06-04T17:50:53.898103Z","shell.execute_reply":"2025-06-04T17:50:53.914563Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def test(model, test_loader, max_text_length, tokenizer):\n    \"\"\"\n    Evaluate and predict model with the test dataloader.\n    Memory-efficient version that processes data in chunks.\n    \n    Args:\n        model: The OCR model to evaluate.\n        test_loader: DataLoader for test dataset.\n        max_text_length: Maximum length of output sequence.\n        tokenizer: Tokenizer for decoding output tokens.\n    \n    Returns:\n        predicts: List of predicted text sequences.\n        gt: List of ground truth text sequences.\n        imgs: List of input images.\n    \"\"\"\n    model.eval()\n    predicts = []\n    gt = []\n    imgs = []\n    device = next(model.parameters()).device\n    \n    # Clear memory before starting\n    if device.type == 'cuda':\n        torch.cuda.empty_cache()\n    \n    chunk_size = 10  # Process 10 samples at a time\n    current_chunk = {'imgs': [], 'preds': [], 'gts': []}\n    \n    with torch.no_grad():\n        try:\n            for batch_idx, batch in enumerate(test_loader):\n                src, trg = batch\n                \n                # Store CPU version of image\n                current_chunk['imgs'].append(src.flatten(0,1).cpu())\n                \n                # Move tensors to device\n                src = src.to(device)\n                trg = trg.to(device)\n                \n                try:\n                    # Forward pass without teacher forcing\n                    output = model(src.float())\n                    \n                    # Free memory\n                    del src\n                    if device.type == 'cuda':\n                        torch.cuda.empty_cache()\n                    \n                    # Ensure output is in (seq_len, batch, vocab_size) format\n                    if output.dim() == 3:\n                        output = output.permute(1, 0, 2)\n                    \n                    # Apply log softmax and get predictions\n                    output = F.log_softmax(output, dim=2)\n                    predictions = output.argmax(dim=2)\n                    \n                    # Free memory\n                    del output\n                    if device.type == 'cuda':\n                        torch.cuda.empty_cache()\n                    \n                    # Process each sequence in the batch\n                    for pred, target in zip(predictions.transpose(0,1), trg):\n                        # Convert prediction to text (handle special tokens)\n                        pred_indices = []\n                        for idx in pred:\n                            token = idx.item()\n                            if token == tokenizer.chars.index('EOS'):\n                                break\n                            if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n                                pred_indices.append(token)\n                        \n                        # Decode prediction\n                        pred_text = tokenizer.decode(pred_indices)\n                        pred_text = pred_text.replace('SOS', '').replace('EOS', '')\n                        current_chunk['preds'].append(pred_text)\n                        \n                        # Convert target to text (handle special tokens)\n                        target_indices = []\n                        for idx in target:\n                            token = idx.item()\n                            if token == tokenizer.chars.index('EOS'):\n                                break\n                            if token > tokenizer.chars.index('EOS'):  # Skip special tokens\n                                target_indices.append(token)\n                        \n                        # Decode target\n                        target_text = tokenizer.decode(target_indices)\n                        target_text = target_text.replace('SOS', '').replace('EOS', '')\n                        current_chunk['gts'].append(target_text)\n                    \n                    # Free memory\n                    del predictions, trg\n                    if device.type == 'cuda':\n                        torch.cuda.empty_cache()\n                    \n                except RuntimeError as e:\n                    print(f\"Error processing batch {batch_idx}: {e}\")\n                    continue\n                \n                # If chunk is full or this is the last batch, append to main lists and clear chunk\n                if len(current_chunk['imgs']) >= chunk_size or batch_idx == len(test_loader) - 1:\n                    imgs.extend(current_chunk['imgs'])\n                    predicts.extend(current_chunk['preds'])\n                    gt.extend(current_chunk['gts'])\n                    \n                    # Clear chunk\n                    current_chunk = {'imgs': [], 'preds': [], 'gts': []}\n                    \n                    # Force garbage collection\n                    import gc\n                    gc.collect()\n                    if device.type == 'cuda':\n                        torch.cuda.empty_cache()\n                \n        except Exception as e:\n            print(f\"Unexpected error during testing: {e}\")\n            # Save what we have so far\n            if current_chunk['imgs']:\n                imgs.extend(current_chunk['imgs'])\n                predicts.extend(current_chunk['preds'])\n                gt.extend(current_chunk['gts'])\n    \n    return predicts, gt, imgs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:53.915998Z","iopub.execute_input":"2025-06-04T17:50:53.916253Z","iopub.status.idle":"2025-06-04T17:50:53.934450Z","shell.execute_reply.started":"2025-06-04T17:50:53.916231Z","shell.execute_reply":"2025-06-04T17:50:53.933838Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Clear any existing cached memory\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\nimport gc\ngc.collect()\n\n# Configure DataLoader for minimal memory usage\ntest_loader = torch.utils.data.DataLoader(\n    DataGenerator(source_path, charset_base, max_text_length, 'test', transform), \n    batch_size=1,  # Keep batch size at 1 for minimum memory usage\n    shuffle=False, \n    num_workers=0,  # Single process loading\n    pin_memory=False,  # Disable pin_memory to reduce memory usage\n    persistent_workers=False,  # Disable persistent workers\n    prefetch_factor=None  # Disable prefetching\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:53.935153Z","iopub.execute_input":"2025-06-04T17:50:53.935412Z","iopub.status.idle":"2025-06-04T17:50:56.428538Z","shell.execute_reply.started":"2025-06-04T17:50:53.935389Z","shell.execute_reply":"2025-06-04T17:50:56.427984Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n\npredicts, gt, imgs = test(model, test_loader, max_text_length, tokenizer)\n\n\n# the part below is causing error \npredicts = list(map(lambda x : x.replace('SOS','').replace('EOS',''),predicts))\ngt = list(map(lambda x : x.replace('SOS','').replace('EOS',''),gt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:50:56.429492Z","iopub.execute_input":"2025-06-04T17:50:56.429765Z","iopub.status.idle":"2025-06-04T17:52:54.284725Z","shell.execute_reply.started":"2025-06-04T17:50:56.429741Z","shell.execute_reply":"2025-06-04T17:52:54.283987Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate = evaluation.ocr_metrics(predicts=predicts,\n                                  ground_truth=gt,)\n \nprint(\"Calculate Character Error Rate {}, Word Error Rate {} and Sequence Error Rate {}\".format(evaluate[0],evaluate[1],evaluate[2]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:52:54.285573Z","iopub.execute_input":"2025-06-04T17:52:54.285815Z","iopub.status.idle":"2025-06-04T17:52:54.300882Z","shell.execute_reply.started":"2025-06-04T17:52:54.285792Z","shell.execute_reply":"2025-06-04T17:52:54.300091Z"}},"outputs":[{"name":"stdout","text":"Calculate Character Error Rate 1.0, Word Error Rate 1.0 and Sequence Error Rate 1.0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom data import preproc as pp\n\ndef show_predictions(imgs, gt, predicts, max_display=10):\n    \"\"\"\n    Display images with their ground truth and predicted text.\n\n    Args:\n        imgs: List of image tensors (C, H, W).\n        gt: List of ground truth strings.\n        predicts: List of predicted strings.\n        max_display: Maximum number of images to display.\n    \"\"\"\n    for i, item in enumerate(imgs[:max_display]):\n        print(\"=\" * 80)\n        img = item.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n        # Convert to grayscale if image has 3 channels\n        if img.shape[2] == 3:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img = pp.adjust_to_see(img)\n        cv2.imshow('Line', img)\n        print(\"Ground truth:\", gt[i])\n        print(\"Prediction :\", predicts[i], \"\\n\")\n        cv2.waitKey(0)\n    cv2.destroyAllWindows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:52:54.302719Z","iopub.execute_input":"2025-06-04T17:52:54.302917Z","iopub.status.idle":"2025-06-04T17:52:54.320891Z","shell.execute_reply.started":"2025-06-04T17:52:54.302902Z","shell.execute_reply":"2025-06-04T17:52:54.320204Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# from src.utils.display_results import show_predictions\n\n# Assuming you have run the test function and obtained these:\n# predicts, gt, imgs = test(model, test_loader, max_text_length, tokenizer)\n\n# Call the display function to show images with predictions and ground truth\nshow_predictions(imgs, gt, predicts, max_display=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:52:54.321599Z","iopub.execute_input":"2025-06-04T17:52:54.321774Z","execution_failed":"2025-06-04T17:52:59.214Z"}},"outputs":[{"name":"stdout","text":"================================================================================\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"! pip install matplotlib seaborn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.083771Z","iopub.status.idle":"2025-05-25T20:55:43.084113Z","shell.execute_reply.started":"2025-05-25T20:55:43.083945Z","shell.execute_reply":"2025-05-25T20:55:43.083959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ntraining_results = joblib.load(output_path + '/training_results.joblib')\n\n# Set Seaborn style\nsns.set(style=\"whitegrid\")\n\n\n# Extract data\ntrain_losses = [r[0] for r in training_results]\nvalid_losses = [r[1] for r in training_results]\ndurations = [r[2] for r in training_results]\nepoch_nums = list(range(1, len(training_results) + 1))\n\n# Plotting\nfig, axs = plt.subplots(1, 3, figsize=(20, 6))\n\n# Training Loss\nsns.lineplot(x=epoch_nums, y=train_losses, marker='o', ax=axs[0], color='blue')\naxs[0].set_title('Training Loss per Epoch')\naxs[0].set_xlabel('Epoch')\naxs[0].set_ylabel('Training Loss')\n\n# Validation Loss\nsns.lineplot(x=epoch_nums, y=valid_losses, marker='o', ax=axs[1], color='green')\naxs[1].set_title('Validation Loss per Epoch')\naxs[1].set_xlabel('Epoch')\naxs[1].set_ylabel('Validation Loss')\n\n# Time per Epoch\nsns.lineplot(x=epoch_nums, y=durations, marker='o', ax=axs[2], color='red')\naxs[2].set_title('Time per Epoch')\naxs[2].set_xlabel('Epoch')\naxs[2].set_ylabel('Time (seconds)')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T20:55:43.085124Z","iopub.status.idle":"2025-05-25T20:55:43.085470Z","shell.execute_reply.started":"2025-05-25T20:55:43.085303Z","shell.execute_reply":"2025-05-25T20:55:43.085319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}